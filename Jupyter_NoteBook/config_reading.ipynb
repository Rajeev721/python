{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "# Creating configParser Object\n",
    "\n",
    "cp = configparser.ConfigParser()\n",
    "\n",
    "# Reading the config files\n",
    "\n",
    "cp.read('D:\\Learning\\config_file_1.ini')\n",
    "\n",
    "#Assigning the parameter values\n",
    "\n",
    "path = cp.get('Paths', 'path')\n",
    "source_file1 = cp.get('Paths', 'source_file1')\n",
    "source_file2 = cp.get('Paths', 'source_file2')\n",
    "Target_file = cp.get('Paths', 'target_file')\n",
    "join_type = cp.get('DB','type_of_join')\n",
    "join_column = cp.get('DB','join_column').strip(\" \")\n",
    "database = cp.get('DB','Database').strip(\" \")\n",
    "target_table = cp.get('DB','target_table').strip(\" \")\n",
    "user_name = cp.get('DB','user_name').strip(\" \")\n",
    "password = cp.get('DB','password').strip(\" \")\n",
    "driver = cp.get('DB', 'driver').strip(\" \")\n",
    "connection_string = cp.get('DB','connection_string').strip(\" \")\n",
    "source_file1_path = os.path.join(path, source_file1)\n",
    "source_file2_path = os.path.join(path, source_file2)\n",
    "Target_file2_path = os.path.join(path, Target_file)\n",
    "df_write = cp.get('DB','df_write')\n",
    "config_table = cp.get('DB','config_table')\n",
    "server_name = cp.get('DB','server')\n",
    "sqldriver = cp.get('DB','sqldriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Config file checkup\").config(\"spark.driver.extraClassPath\",\"D:\\spark\\spark-3.1.2-bin-hadoop2.7\\jars\\mssql-jdbc-9.2.1.jre11.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df1 = spark.read.csv(source_file1_path,header=True,inferSchema=True)\n",
    "source_df2 = spark.read.csv(source_file2_path,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'source_df1.join(source_df2,'+'source_df1.'+ join_column + \" == source_df2.\"+ join_column+ \",\" +\"'\"+ join_type +\"')\"\n",
    "# a = \"source_df1.join(source_df2,source_df1.{0} == source_df2.{0},'{1}')\".format(join_column,join_type)\n",
    "\n",
    "b = 'join_df.write.mode(\"append\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + target_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "# b = 'join_df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"{0}\").option(\"dbtable\",\"{1}\").option(\"user\",\"{2}\").option(\"password\",\"{3}\").option(\"driver\",\"{4}\").save()'.format(connection_string,target_table,user_name,password,driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Sepal_Length: double (nullable = true)\n",
      " |-- Sepal_Width: double (nullable = true)\n",
      " |-- Petal_Length: double (nullable = true)\n",
      " |-- Petal_Width: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "join_df = eval(a).select(source_df1.ID,source_df1.Sepal_Length,source_df1.Sepal_Width,source_df2.Petal_Length,source_df2.Petal_Width,source_df2.Species)\n",
    "\n",
    "#a = join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\")\n",
    "#a.show()\n",
    "print(join_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\").write.mode(\"append\").parquet(\"D:\\Learning\\sample_file1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "| ID|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|  4|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|  5|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|  6|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|  7|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|  8|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|  9|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "| 10|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parquet = spark.read.parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(b)\n",
    "\n",
    "# Following parameter is stored in config file to execute that we are using exec\n",
    "#df_write = 'join_df.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + target_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "#\n",
    "\n",
    "exec(df_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID_NUM: string (nullable = true)\n",
      " |-- SOURCE_PATH: string (nullable = true)\n",
      " |-- SOURCE_FILE_1: string (nullable = true)\n",
      " |-- SOURCE_FILE_2: string (nullable = true)\n",
      " |-- TARGET_FILE: string (nullable = true)\n",
      " |-- DATABASE_N: string (nullable = true)\n",
      " |-- CONNECTION_STRING: string (nullable = true)\n",
      " |-- DRIVER: string (nullable = true)\n",
      " |-- SOURCE_TABLES: string (nullable = true)\n",
      " |-- TARGET_TABLE: string (nullable = true)\n",
      " |-- TYPE_OF_JOIN: string (nullable = true)\n",
      " |-- JOIN_COLUMN: string (nullable = true)\n",
      " |-- USER_NAME_V: string (nullable = true)\n",
      " |-- PASSWORD_V: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Uploading config details into Table'''\n",
    "df_config_load = spark.read.csv('D:\\Learning\\config_file_load.csv',header=True)\n",
    "df_config_load1 = df_config_load.select(col(\"ID_NUM\").cast('int'),\"SOURCE_PATH\",\"SOURCE_FILE_1\",\"SOURCE_FILE_2\",\"TARGET_FILE\",\"DATABASE_N\",\"CONNECTION_STRING\",\"DRIVER\",\"SOURCE_TABLES\",\"TARGET_TABLE\",\"TYPE_OF_JOIN\",\"JOIN_COLUMN\",\"USER_NAME_V\",\"PASSWORD_V\")\n",
    "df_config_load.printSchema()\n",
    "\n",
    "config_upload = 'df_config_load1.write.mode(\"overwrite\").format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + config_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").save()'\n",
    "eval(config_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---------------+---------------+-----------+-------------+--------------------+--------------------+-------------+-----------------+------------+-----------+-----------+----------+\n",
      "|ID_NUM|         SOURCE_PATH|  SOURCE_FILE_1|  SOURCE_FILE_2|TARGET_FILE|   DATABASE_N|   CONNECTION_STRING|              DRIVER|SOURCE_TABLES|     TARGET_TABLE|TYPE_OF_JOIN|JOIN_COLUMN|USER_NAME_V|PASSWORD_V|\n",
      "+------+--------------------+---------------+---------------+-----------+-------------+--------------------+--------------------+-------------+-----------------+------------+-----------+-----------+----------+\n",
      "|     1|D:\\Learning\\DataS...|iris_merge1.csv|iris_merge2.csv|     sample|rajeev_sql_db|jdbc:sqlserver://...|com.microsoft.sql...|     employee|SalesLT.iris_data|       inner|         ID|     rajeev|Mfu87@lqp0|\n",
      "+------+--------------------+---------------+---------------+-----------+-------------+--------------------+--------------------+-------------+-----------------+------------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Reading Config details from Table '''\n",
    "config_load = 'spark.read.format(\"jdbc\").option(\"url\",\"' + connection_string + '\").option(\"dbtable\",\"' + config_table + '\").option(\"user\",\"' + user_name +'\").option(\"password\",\"' + password + '\").option(\"driver\",\"' + driver + '\").load()'\n",
    "config_load_df = eval(config_load)\n",
    "config_load_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data from config file\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Config Table\").config(\"spark.driver.extraClassPath\",\"D:\\spark\\spark-3.1.2-bin-hadoop2.7\\jars\\mssql-jdbc-9.2.1.jre11.jar\").getOrCreate()\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "conn = pyodbc.connect('DRIVER='+sqldriver+';SERVER='+server_name+';PORT=1433;DATABASE='+database+';UID='+user_name+';PWD='+ password)\n",
    "\n",
    "#'pyodbc.connect(\"DRIVER=\"{0}\";SERVER=\"{1}\";PORT=1433;DATABASE=\"{2}\";UID=\"{3}\";PWD=\"{4})'.format(sqldriver,server_name,database,user_name,password)\n",
    "\n",
    "\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('select * from SalesLT.config_table where ID_NUM = 1')\n",
    "    row = cursor.fetchone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "D:\\Learning\\DataSet\\iris\\merge\n",
      "iris_merge1.csv\n",
      "iris_merge2.csv\n",
      "sample\n",
      "rajeev_sql_db\n",
      "jdbc:sqlserver://rajeev-sql-server.database.windows.net:1433;database=rajeev_sql_db;\n",
      "com.microsoft.sqlserver.jdbc.SQLServerDriver\n",
      "employee\n",
      "SalesLT.iris_data\n",
      "inner\n",
      "ID\n",
      "rajeev\n",
      "Mfu87@lqp0\n"
     ]
    }
   ],
   "source": [
    "ID_NUM = row[0]\n",
    "SOURCE_PATH = row[1]\n",
    "SOURCE_FILE_1 = row[2]\n",
    "SOURCE_FILE_2 = row[3]\n",
    "TARGET_FILE = row[4]\n",
    "DATABASE_N = row[5]\n",
    "CONNECTION_STRING = row[6]\n",
    "DRIVER = row[7]\n",
    "SOURCE_TABLES = row[8]\n",
    "TARGET_TABLE = row[9]\n",
    "TYPE_OF_JOIN = row[10]\n",
    "JOIN_COLUMN = row[11]\n",
    "USER_NAME_V = row[12]\n",
    "PASSWORD_V = row[13]\n",
    "\n",
    "print(ID_NUM)\n",
    "print(SOURCE_PATH)\n",
    "print(SOURCE_FILE_1)\n",
    "print(SOURCE_FILE_2)\n",
    "print(TARGET_FILE)\n",
    "print(DATABASE_N)\n",
    "print(CONNECTION_STRING)\n",
    "print(DRIVER)\n",
    "print(SOURCE_TABLES)\n",
    "print(TARGET_TABLE)\n",
    "print(TYPE_OF_JOIN)\n",
    "print(JOIN_COLUMN)\n",
    "print(USER_NAME_V)\n",
    "print(PASSWORD_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df1 = spark.read.csv(source_file1_path,header=True,inferSchema=True)\n",
    "source_df2 = spark.read.csv(source_file2_path,header=True,inferSchema=True)\n",
    "\n",
    "a = 'source_df1.join(source_df2,'+'source_df1.'+ JOIN_COLUMN + \" == source_df2.\"+ JOIN_COLUMN+ \",\" +\"'\"+ TYPE_OF_JOIN +\"')\"\n",
    "b = 'join_df.write.mode(\"append\").format(\"jdbc\").option(\"url\",\"' + CONNECTION_STRING + '\").option(\"dbtable\",\"' + TARGET_TABLE + '\").option(\"user\",\"' + USER_NAME_V +'\").option(\"password\",\"' + PASSWORD_V + '\").option(\"driver\",\"' + DRIVER + '\").save()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = eval(a).select(source_df1.ID,source_df1.Sepal_Length,source_df1.Sepal_Width,source_df2.Petal_Length,source_df2.Petal_Width,source_df2.Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-------+\n",
      "| ID|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|  4|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|  5|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|  6|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|  7|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|  8|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|  9|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "| 10|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "+---+------------+-----------+------------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df.select(\"ID\",\"Sepal_Length\",\"Sepal_Width\",\"Petal_Length\",\"Petal_Width\",\"Species\").write.mode(\"append\").parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet = spark.read.parquet(\"D:\\Learning\\sample_file1\")\n",
    "\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
